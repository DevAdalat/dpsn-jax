model:
  d_model: 512
  num_layers: 8
  num_memory_slots: 25000  # Increased to boost pool size
  min_k: 64
  max_k: 256

training:
  batch_size: 8
  seq_len: 64
  learning_rate: 0.0003
  steps: 200
  log_every_steps: 1
  save_every_steps: 50
  seed: 42
  workdir: "checkpoints/dpsn_large"

data:
  path: "data/input.txt"
  vocab_path: "data/vocab.json"
