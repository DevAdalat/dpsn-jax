model:
  d_model: 1024
  num_layers: 16
  num_memory_slots: 32768
  min_k: 32
  max_k: 128
  router_dim: 256

training:
  batch_size: 2
  seq_len: 128
  learning_rate: 3.0e-4
  steps: 1000
  log_every_steps: 10
  save_every_steps: 100
  seed: 42
  workdir: "checkpoints_1b"

data:
  huggingface_dataset: "VisionTheta/fineweb-1B"
  tokenizer_name: "zai-org/GLM-4.7"
  streaming: true
  dataset_column_name: "text"
